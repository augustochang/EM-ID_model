{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Red Pitaya at 192.168.8.214\n",
      "Classification result: {'timestamp': '2024-12-02T15:05:24.190205', 'empty': 1.0, 'ipad': 0.0, 'lamp': 0.0}\n",
      "Classification result: {'timestamp': '2024-12-02T15:05:27.705742', 'empty': 1.0, 'ipad': 0.0, 'lamp': 0.0}\n",
      "\n",
      "Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import redpitaya_scpi as scpi\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Function to load configurations from config.json\n",
    "def load_config(config_path='config.json'):\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file '{config_path}' not found.\")\n",
    "    \n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    \n",
    "    required_keys = [\"ip_address\", \"decimation_factor\", \"scaler_path\", \"model_path\"]\n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise KeyError(f\"Missing required configuration key: '{key}'\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# Function to compute statistical features\n",
    "def compute_statistical_features(signal):\n",
    "    \"\"\"\n",
    "    Compute statistical features from the Welch power spectrum of a signal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        features = {\n",
    "            'entropy': np.sum(-signal * np.log2(signal + 1e-12)),  # Avoid log(0)\n",
    "            'skewness': pd.Series(signal).skew(),\n",
    "            'interquartile_range': np.percentile(signal, 75) - np.percentile(signal, 25),\n",
    "            'kurtosis': pd.Series(signal).kurtosis(),\n",
    "            'percentile_75': np.percentile(signal, 75),\n",
    "            'range': np.ptp(signal),\n",
    "            'maximum': np.max(signal),\n",
    "            'median': np.median(signal),\n",
    "            'percentile_90': np.percentile(signal, 90),\n",
    "            'mean_absolute_deviation': np.mean(np.abs(signal - np.mean(signal)))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing features: {e}\")\n",
    "        features = {key: np.nan for key in [\n",
    "            'entropy', 'skewness', 'interquartile_range', 'kurtosis', 'percentile_75',\n",
    "            'range', 'maximum', 'median', 'percentile_90', 'mean_absolute_deviation']}\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# Function to process live data and classify using the KNN model\n",
    "def classify_live_data(signal, scaler, model, decimation_factor):\n",
    "    \"\"\"\n",
    "    Process live data from Red Pitaya and classify it using the trained model.\n",
    "    \"\"\"\n",
    "    # Compute Welch power spectrum\n",
    "    frequencies, power_spectrum = welch(signal, fs=125e6 / decimation_factor, nperseg=1024)\n",
    "    power_spectrum = power_spectrum / np.sum(power_spectrum)  # Normalize to get probabilities\n",
    "    \n",
    "    # Compute statistical features\n",
    "    features = compute_statistical_features(power_spectrum)\n",
    "    feature_array = np.array(list(features.values())).reshape(1, -1)\n",
    "    \n",
    "    # Scale features and classify\n",
    "    scaled_features = scaler.transform(feature_array)\n",
    "    probabilities = model.predict_proba(scaled_features)[0]\n",
    "    \n",
    "    # Map probabilities to device labels\n",
    "    device_labels = model.classes_\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    classification_result = {'timestamp': timestamp}\n",
    "    classification_result.update({label: prob for label, prob in zip(device_labels, probabilities)})\n",
    "    \n",
    "    return classification_result\n",
    "\n",
    "\n",
    "# Main data collection and classification function\n",
    "def collect_and_classify(config):\n",
    "    # Extract configurations\n",
    "    ip_address = config['ip_address']\n",
    "    decimation_factor = config['decimation_factor']\n",
    "    scaler_path = config['scaler_path']\n",
    "    model_path = config['model_path']\n",
    "    \n",
    "    # Load scaler and model\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Initialize Red Pitaya connection\n",
    "    try:\n",
    "        rp_s = scpi.scpi(ip_address)\n",
    "        print(f\"Connected to Red Pitaya at {ip_address}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Red Pitaya at {ip_address}: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Configure Red Pitaya\n",
    "            try:\n",
    "                rp_s.tx_txt('ACQ:RST')  # Reset acquisition\n",
    "                rp_s.tx_txt(f'ACQ:DEC {decimation_factor}')  # Set decimation factor\n",
    "                rp_s.tx_txt('ACQ:START')  # Start acquisition\n",
    "                time.sleep(1)  # Wait for data capture\n",
    "            except Exception as e:\n",
    "                print(f\"Error configuring Red Pitaya: {e}\")\n",
    "                time.sleep(2)\n",
    "                continue  # Skip to next iteration\n",
    "            \n",
    "            # Capture data from channel 1\n",
    "            try:\n",
    "                rp_s.tx_txt('ACQ:SOUR1:DATA?')\n",
    "                raw_data_ch1 = rp_s.rx_txt().strip('{}\\n\\r').split(',')\n",
    "                signal_ch1 = np.array([float(x) for x in raw_data_ch1])\n",
    "                signal_ch1 -= np.mean(signal_ch1)  # Remove DC component\n",
    "            except Exception as e:\n",
    "                print(f\"Error acquiring data from channel 1: {e}\")\n",
    "                time.sleep(1)\n",
    "                continue  # Skip to next iteration\n",
    "            \n",
    "            # Classify live data\n",
    "            if signal_ch1.size > 0:\n",
    "                try:\n",
    "                    result = classify_live_data(signal_ch1, scaler, model, decimation_factor)\n",
    "                    print(f\"Classification result: {result}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error classifying live data: {e}\")\n",
    "            else:\n",
    "                print(\"No valid data captured from channel 1.\")\n",
    "            \n",
    "            # Optional: control loop frequency\n",
    "            time.sleep(2)  # Adjust the delay as needed\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config = load_config('config.json')\n",
    "    except (FileNotFoundError, KeyError) as e:\n",
    "        print(f\"Configuration Error: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    collect_and_classify(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tester_arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
